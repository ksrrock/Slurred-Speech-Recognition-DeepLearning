{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BTP Part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BukkGwnQ81U",
        "outputId": "2f6dcdc2-2176-4807-bef8-d90e38cb37c2"
      },
      "source": [
        "pip install torchaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: torch==1.9.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1->torchaudio) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKVGDRB0fW_Q"
      },
      "source": [
        "## Importing all neccessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvuzLADtfVwn"
      },
      "source": [
        "import warnings\n",
        "warnings. filterwarnings(\"ignore\")\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torchaudio import transforms as tfs\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from torch.nn import Sequential as sq\n",
        "from torch.nn import Linear as lnr\n",
        "from torch import argmax as agm\n",
        "from torch.nn import Module as ml\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhdCAHRHfaR1"
      },
      "source": [
        "## Evaluation Metrics implementation (WER and CER)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N94pVVHDfeVE"
      },
      "source": [
        "\n",
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "  \n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        " \n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer\n",
        "\n",
        "class TextTransform:\n",
        "    def __init__(self):\n",
        "        self.char_map={\"'\": 0, '': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
        "        self.index_map={0: \"'\", 1: ' ', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n",
        "        \n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    \n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('', ' ')\n",
        "\n",
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S03454e2nQ9p"
      },
      "source": [
        "## The NVIDIA System Management Interface (management and monitoring of NVIDIA GPU devices)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrzOZ5BgX8Ps",
        "outputId": "8233f20f-4743-4642-9c1f-da5fc7a56dce"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM0mA41WneNT"
      },
      "source": [
        "# **CNN layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpObb3vXX_El"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(2, 3).contiguous() \n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x \n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x \n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUGjv3ounj3_"
      },
      "source": [
        "# **Gated Recurrent Unit layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMqmUQLJYBI-"
      },
      "source": [
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cupyNOdlno-Z"
      },
      "source": [
        "# Final Automatic Speech Recognition Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kIVttAsYDb5"
      },
      "source": [
        "\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2) \n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  \n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3]) \n",
        "        x = x.transpose(1, 2) \n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y41lkdZ-noVh"
      },
      "source": [
        "# Training and Testing Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ8oZecBYGFi"
      },
      "source": [
        "class IterMeter(object):\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(spectrograms) \n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "\n",
        "    \n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        iter_meter.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  \n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "  \n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "    # filename='./best_fit_model'+str(epoch)+'.h5'\n",
        "    # torch.save(model,filename)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGj7rLijn4Q-"
      },
      "source": [
        "# Hyperparameters of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMG9JxM4YHaA"
      },
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 5\n",
        "epochs = 10\n",
        "train_url = \"train-clean-100\"\n",
        "test_url = \"test-clean\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pUOYYKHYiCC"
      },
      "source": [
        "if not os.path.isdir(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4KvaBDgYKln",
        "outputId": "b9391c07-500f-4861-adb1-82037b3329f9"
      },
      "source": [
        "hparams = {\n",
        "    \"n_cnn_layers\": 3,\n",
        "    \"n_rnn_layers\": 5,\n",
        "    \"rnn_dim\": 512,\n",
        "    \"n_class\": 29,\n",
        "    \"n_feats\": 128,\n",
        "    \"stride\":2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs\n",
        "}\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                            batch_size=hparams['batch_size'],\n",
        "                            shuffle=True,\n",
        "                            collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                            **kwargs)\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                            batch_size=hparams['batch_size'],\n",
        "                            shuffle=False,\n",
        "                            collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                            **kwargs)\n",
        "\n",
        "model = SpeechRecognitionModel(\n",
        "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "print(model)\n",
        "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "criterion = nn.CTCLoss(blank=28).to(device)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], steps_per_epoch=int(len(train_loader)),epochs=hparams['epochs'],anneal_strategy='linear')\n",
        "\n",
        "iter_meter = IterMeter()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n",
            "Num Model Parameters 23705373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxftuqFfo_tO"
      },
      "source": [
        "# Code for model training and testing (Optional)\n",
        "\n",
        "\n",
        "> This code snippet should only be used for base model training.\n",
        "\n",
        "> Since we already have our based model trained , we will skip this part and load our trained model directly\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C6FJgkro8I3"
      },
      "source": [
        "# for epoch in range(1, epochs+1):\n",
        "#     train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
        "#     test(model, device, test_loader, criterion, epoch, iter_meter)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX0uozFvn--5"
      },
      "source": [
        "# Loading the best fit model\n",
        "\n",
        "\n",
        "*   The base model for slurred speech Recognition has been trained on kaggle for 10 epoch.\n",
        "*   It has been trained on Librispeech Corpus Speech Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGALiIQhZMwO"
      },
      "source": [
        "loaded_model = torch.load('/content/drive/MyDrive/BTP Datasets/Best_fit_model/best_fit_model10.h5',map_location=torch.device('cpu'))\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh6gIyA4YgpQ",
        "outputId": "3bca0a29-ea4e-4db5-b347-dd6e3bbde533"
      },
      "source": [
        "print(loaded_model)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SpeechRecognitionModel(\n",
            "  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "  (rescnn_layers): Sequential(\n",
            "    (0): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ResidualCNN(\n",
            "      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      (layer_norm1): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (layer_norm2): CNNLayerNorm(\n",
            "        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  (birnn_layers): Sequential(\n",
            "    (0): BidirectionalGRU(\n",
            "      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (1): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (2): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (3): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (4): BidirectionalGRU(\n",
            "      (BiGRU): GRU(1024, 512, bidirectional=True)\n",
            "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): GELU()\n",
            "    (2): Dropout(p=0.1, inplace=False)\n",
            "    (3): Linear(in_features=512, out_features=29, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEXebGeypl03"
      },
      "source": [
        "# Testing our loaded base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUkhKqGGpxol",
        "outputId": "b74420dd-33f0-4d3d-fc09-6523a8ceaf2a"
      },
      "source": [
        "test(loaded_model, device, test_loader, criterion, 1, iter_meter)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 0.5171, Average CER: 0.119907 Average WER: 0.1413\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p8C5mpLbm20"
      },
      "source": [
        "import os\n",
        "from typing import Tuple, Union\n",
        "from pathlib import Path\n",
        "import torchaudio\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "def load_librispeech_item(fileid,path,ext_audio,ext_txt):\n",
        "    speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n",
        "\n",
        "    file_text = speaker_id + \"-\" + chapter_id + ext_txt\n",
        "    file_text = os.path.join(path, speaker_id, chapter_id, file_text)\n",
        "\n",
        "    fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n",
        "    file_audio = fileid_audio + ext_audio\n",
        "    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n",
        "\n",
        "    waveform, sample_rate = torchaudio.load(file_audio)\n",
        "\n",
        "    with open(file_text) as ft:\n",
        "        for line in ft:\n",
        "            fileid_text, utterance = line.strip().split(\" \", 1)\n",
        "            if fileid_audio == fileid_text:\n",
        "                break\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n",
        "\n",
        "    return (waveform,sample_rate,utterance,int(speaker_id),int(chapter_id),int(utterance_id),)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNeuTm-IFr8x"
      },
      "source": [
        "## Custom class for loading Slurred Speech Test Dataset in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1AWy9snF1WZ"
      },
      "source": [
        "class TestDataset(Dataset): \n",
        "    _ext_txt = \".trans.txt\"\n",
        "    _ext_audio = \".flac\"\n",
        "    def __init__(self):\n",
        "        self._path=\"/content/drive/MyDrive/BTP Datasets/SlurredSpeech_Dataset_Test\"\n",
        "        self._walker=['1-2-0000', '1-2-0001', '1-2-0002', '1-2-0003', '1-2-0004', '1-2-0005', '1-2-0006', '1-2-0007', '1-2-0008', '1-2-0009', '1-2-0010', '1-2-0011', '1-2-0012', '1-2-0013', '1-2-0014', '1-2-0015', '1-2-0016', '1-2-0017', '1-2-0018', '1-2-0019', '1-2-0020', '1-2-0021', '1-2-0022', '1-2-0023', '1-2-0024', '1-2-0025', '1-2-0026', '1-2-0027', '1-2-0028', '1-2-0029', '1-2-0030', '1-2-0031', '1-2-0032', '1-2-0033', '1-2-0034', '1-2-0035', '1-2-0036', '1-2-0037', '1-2-0038', '1-2-0039', '1-2-0040', '1-2-0041', '1-2-0042', '1-2-0043', '1-2-0044', '1-2-0045', '1-2-0046', '1-2-0047', '1-2-0048', '1-2-0049', '1-2-0050', '1-2-0051', '1-2-0052', '1-2-0053', '1-2-0054', '1-2-0055', '1-2-0056', '1-2-0057', '1-2-0058', '1-2-0059', '1-2-0060', '1-2-0061', '1-2-0062', '1-2-0063', '1-2-0064', '1-2-0065', '1-2-0066', '1-2-0067', '1-2-0068', '1-2-0069', '1-2-0070', '1-2-0071', '1-2-0072', '1-2-0073', '1-2-0074', '1-2-0075', '1-2-0076', '1-2-0077', '1-2-0078', '1-2-0079', '1-2-0080', '1-2-0081', '1-2-0082', '1-2-0083', '1-2-0084', '1-2-0085', '1-2-0086', '1-2-0087', '1-2-0088', '1-2-0089', '1-2-0090', '1-2-0091', '1-2-0092', '1-2-0093', '1-2-0094', '1-2-0095', '1-2-0096', '1-2-0097', '1-2-0098', '1-2-0099']\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        fileid = self._walker[n]\n",
        "        return load_librispeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._walker)   \n",
        "    \n",
        "    \n",
        "testDataset_slurred_speech=TestDataset()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-yHQfJhF4Is"
      },
      "source": [
        "## Custom class for loading Slurred Speech Train Dataset in Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32vqme6tF_9D"
      },
      "source": [
        "class TrainDataset(Dataset): \n",
        "    _ext_txt = \".trans.txt\"\n",
        "    _ext_audio = \".flac\"\n",
        "    def __init__(self):\n",
        "        self._path=\"/content/drive/MyDrive/BTP Datasets/SlurredSpeech_Dataset_Train\"\n",
        "        self._walker=['1-2-0000', '1-2-0001', '1-2-0002', '1-2-0003', '1-2-0004', '1-2-0005', '1-2-0006', '1-2-0007', '1-2-0008', '1-2-0009', '1-2-0010', '1-2-0011', '1-2-0012', '1-2-0013', '1-2-0014', '1-2-0015', '1-2-0016', '1-2-0017', '1-2-0018', '1-2-0019', '1-2-0020', '1-2-0021', '1-2-0022', '1-2-0023', '1-2-0024', '1-2-0025', '1-2-0026', '1-2-0027', '1-2-0028', '1-2-0029', '1-2-0030', '1-2-0031', '1-2-0032', '1-2-0033', '1-2-0034', '1-2-0035', '1-2-0036', '1-2-0037', '1-2-0038', '1-2-0039', '1-2-0040', '1-2-0041', '1-2-0042', '1-2-0043', '1-2-0044', '1-2-0045', '1-2-0046', '1-2-0047', '1-2-0048', '1-2-0049', '1-2-0050', '1-2-0051', '1-2-0052', '1-2-0053', '1-2-0054', '1-2-0055', '1-2-0056', '1-2-0057', '1-2-0058', '1-2-0059', '1-2-0060', '1-2-0061', '1-2-0062', '1-2-0063', '1-2-0064', '1-2-0065', '1-2-0066', '1-2-0067', '1-2-0068', '1-2-0069', '1-2-0070', '1-2-0071', '1-2-0072', '1-2-0073', '1-2-0074', '1-2-0075', '1-2-0076', '1-2-0077', '1-2-0078', '1-2-0079', '1-2-0080', '1-2-0081', '1-2-0082', '1-2-0083', '1-2-0084', '1-2-0085', '1-2-0086', '1-2-0087', '1-2-0088', '1-2-0089', '1-2-0090', '1-2-0091', '1-2-0092', '1-2-0093', '1-2-0094', '1-2-0095', '1-2-0096', '1-2-0097', '1-2-0098', '1-2-0099']\n",
        "\n",
        "    def __getitem__(self, n):\n",
        "        fileid = self._walker[n]\n",
        "        return load_librispeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._walker)   \n",
        "    \n",
        "    \n",
        "trainDataset_slurred_speech=TrainDataset()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK8WOvO3bnAo"
      },
      "source": [
        "\n",
        "testDataLoaderSlurred = data.DataLoader(dataset=testDataset_slurred_speech,batch_size=hparams['batch_size'],shuffle=False,collate_fn=lambda x: data_processing(x, 'valid'),**kwargs)\n",
        "trainDataLoderSlurred = data.DataLoader(dataset=trainDataset_slurred_speech,batch_size=hparams['batch_size'],shuffle=False,collate_fn=lambda x: data_processing(x, 'valid'),**kwargs)\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ShzeIQjw7Zw"
      },
      "source": [
        "# Testing our base model before fine tuning on slurred speech dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRpIbdbfbnGl",
        "outputId": "8102389c-1b78-4bf4-d796-ce7498008b24"
      },
      "source": [
        "test(loaded_model, device, testDataLoaderSlurred, criterion, 1, iter_meter)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 4.3672, Average CER: 0.638808 Average WER: 0.6565\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS6xjIOzxQLz"
      },
      "source": [
        "# Applying transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3pvzxRrBvF7"
      },
      "source": [
        "def transfer_learning_type1(pre_trained_model):\n",
        "  hparams = {\"learning_rate\": 5e-4,\"batch_size\": 5,\"epochs\": 20}\n",
        "  opti = optim.AdamW(pre_trained_model.parameters(), hparams['learning_rate'])\n",
        "  crit = nn.CTCLoss(blank=28).to(device)\n",
        "  sche = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], steps_per_epoch=int(len(trainDataLoderSlurred)),epochs=hparams['epochs'],anneal_strategy='linear')\n",
        "  iter = IterMeter()\n",
        "  for epoch in range(1, epochs+1):\n",
        "    train(pre_trained_model, device, trainDataLoderSlurred, crit, opti, sche, epoch, iter)\n",
        "    test(pre_trained_model, device, testDataLoaderSlurred, crit, epoch, iter)\n",
        "\n",
        "transfer_learning_type1(loaded_model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX1-zLBXBI3H",
        "outputId": "dd2db7f6-b7a2-46ae-e6e3-de8e3ffa7a3c"
      },
      "source": [
        "params=loaded_model.state_dict()\n",
        "print(params.keys())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['cnn.weight', 'cnn.bias', 'rescnn_layers.0.cnn1.weight', 'rescnn_layers.0.cnn1.bias', 'rescnn_layers.0.cnn2.weight', 'rescnn_layers.0.cnn2.bias', 'rescnn_layers.0.layer_norm1.layer_norm.weight', 'rescnn_layers.0.layer_norm1.layer_norm.bias', 'rescnn_layers.0.layer_norm2.layer_norm.weight', 'rescnn_layers.0.layer_norm2.layer_norm.bias', 'rescnn_layers.1.cnn1.weight', 'rescnn_layers.1.cnn1.bias', 'rescnn_layers.1.cnn2.weight', 'rescnn_layers.1.cnn2.bias', 'rescnn_layers.1.layer_norm1.layer_norm.weight', 'rescnn_layers.1.layer_norm1.layer_norm.bias', 'rescnn_layers.1.layer_norm2.layer_norm.weight', 'rescnn_layers.1.layer_norm2.layer_norm.bias', 'rescnn_layers.2.cnn1.weight', 'rescnn_layers.2.cnn1.bias', 'rescnn_layers.2.cnn2.weight', 'rescnn_layers.2.cnn2.bias', 'rescnn_layers.2.layer_norm1.layer_norm.weight', 'rescnn_layers.2.layer_norm1.layer_norm.bias', 'rescnn_layers.2.layer_norm2.layer_norm.weight', 'rescnn_layers.2.layer_norm2.layer_norm.bias', 'fully_connected.weight', 'fully_connected.bias', 'birnn_layers.0.BiGRU.weight_ih_l0', 'birnn_layers.0.BiGRU.weight_hh_l0', 'birnn_layers.0.BiGRU.bias_ih_l0', 'birnn_layers.0.BiGRU.bias_hh_l0', 'birnn_layers.0.BiGRU.weight_ih_l0_reverse', 'birnn_layers.0.BiGRU.weight_hh_l0_reverse', 'birnn_layers.0.BiGRU.bias_ih_l0_reverse', 'birnn_layers.0.BiGRU.bias_hh_l0_reverse', 'birnn_layers.0.layer_norm.weight', 'birnn_layers.0.layer_norm.bias', 'birnn_layers.1.BiGRU.weight_ih_l0', 'birnn_layers.1.BiGRU.weight_hh_l0', 'birnn_layers.1.BiGRU.bias_ih_l0', 'birnn_layers.1.BiGRU.bias_hh_l0', 'birnn_layers.1.BiGRU.weight_ih_l0_reverse', 'birnn_layers.1.BiGRU.weight_hh_l0_reverse', 'birnn_layers.1.BiGRU.bias_ih_l0_reverse', 'birnn_layers.1.BiGRU.bias_hh_l0_reverse', 'birnn_layers.1.layer_norm.weight', 'birnn_layers.1.layer_norm.bias', 'birnn_layers.2.BiGRU.weight_ih_l0', 'birnn_layers.2.BiGRU.weight_hh_l0', 'birnn_layers.2.BiGRU.bias_ih_l0', 'birnn_layers.2.BiGRU.bias_hh_l0', 'birnn_layers.2.BiGRU.weight_ih_l0_reverse', 'birnn_layers.2.BiGRU.weight_hh_l0_reverse', 'birnn_layers.2.BiGRU.bias_ih_l0_reverse', 'birnn_layers.2.BiGRU.bias_hh_l0_reverse', 'birnn_layers.2.layer_norm.weight', 'birnn_layers.2.layer_norm.bias', 'birnn_layers.3.BiGRU.weight_ih_l0', 'birnn_layers.3.BiGRU.weight_hh_l0', 'birnn_layers.3.BiGRU.bias_ih_l0', 'birnn_layers.3.BiGRU.bias_hh_l0', 'birnn_layers.3.BiGRU.weight_ih_l0_reverse', 'birnn_layers.3.BiGRU.weight_hh_l0_reverse', 'birnn_layers.3.BiGRU.bias_ih_l0_reverse', 'birnn_layers.3.BiGRU.bias_hh_l0_reverse', 'birnn_layers.3.layer_norm.weight', 'birnn_layers.3.layer_norm.bias', 'birnn_layers.4.BiGRU.weight_ih_l0', 'birnn_layers.4.BiGRU.weight_hh_l0', 'birnn_layers.4.BiGRU.bias_ih_l0', 'birnn_layers.4.BiGRU.bias_hh_l0', 'birnn_layers.4.BiGRU.weight_ih_l0_reverse', 'birnn_layers.4.BiGRU.weight_hh_l0_reverse', 'birnn_layers.4.BiGRU.bias_ih_l0_reverse', 'birnn_layers.4.BiGRU.bias_hh_l0_reverse', 'birnn_layers.4.layer_norm.weight', 'birnn_layers.4.layer_norm.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDR-tXqi2j0N",
        "outputId": "6341bd5c-d619-434d-cad5-f0510d79bb95"
      },
      "source": [
        "def transfer_learning(pre_trained_model):\n",
        "  learning_rate = 5e-4\n",
        "  batch_size = 5\n",
        "  epochs = 10\n",
        "  params=loaded_model.state_dict()\n",
        "  print(params.keys())\n",
        "  loaded_model.cnn.weight.requires_grad=False\n",
        "  loaded_model.rescnn_layers[0].cnn1.weight.requires_grad=False\n",
        "  loaded_model.rescnn_layers[0].cnn2.weight.requires_grad=False\n",
        "  loaded_model.rescnn_layers[0].layer_norm1.layer_norm.weight.requires_grad=False\n",
        "  loaded_model.rescnn_layers[0].layer_norm2.layer_norm.weight.requires_grad=False\n",
        "  loaded_model.rescnn_layers[1].cnn1.weight.requires_grad=False\n",
        "  loaded_model.rescnn_layers[1].cnn2.weight.requires_grad=False\n",
        "  hparams = {\"n_cnn_layers\": 3,\"n_rnn_layers\": 5,\"rnn_dim\": 512,\"n_class\": 29,\"n_feats\": 128,\"stride\":2,\"dropout\": 0.1,\"learning_rate\": learning_rate,\"batch_size\": batch_size,\"epochs\": epochs}\n",
        "  opti = optim.AdamW(pre_trained_model.parameters(), hparams['learning_rate'])\n",
        "  crit = nn.CTCLoss(blank=28).to(device)\n",
        "  sche = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], steps_per_epoch=int(len(trainDataLoderSlurred)),epochs=hparams['epochs'],anneal_strategy='linear')\n",
        "  iter = IterMeter()\n",
        "  for epoch in range(1, epochs+1):\n",
        "    train(pre_trained_model, device, trainDataLoderSlurred, crit, opti, sche, epoch, iter)\n",
        "    test(pre_trained_model, device, testDataLoaderSlurred, crit, epoch, iter)\n",
        "\n",
        "transfer_learning(loaded_model)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['cnn.weight', 'cnn.bias', 'rescnn_layers.0.cnn1.weight', 'rescnn_layers.0.cnn1.bias', 'rescnn_layers.0.cnn2.weight', 'rescnn_layers.0.cnn2.bias', 'rescnn_layers.0.layer_norm1.layer_norm.weight', 'rescnn_layers.0.layer_norm1.layer_norm.bias', 'rescnn_layers.0.layer_norm2.layer_norm.weight', 'rescnn_layers.0.layer_norm2.layer_norm.bias', 'rescnn_layers.1.cnn1.weight', 'rescnn_layers.1.cnn1.bias', 'rescnn_layers.1.cnn2.weight', 'rescnn_layers.1.cnn2.bias', 'rescnn_layers.1.layer_norm1.layer_norm.weight', 'rescnn_layers.1.layer_norm1.layer_norm.bias', 'rescnn_layers.1.layer_norm2.layer_norm.weight', 'rescnn_layers.1.layer_norm2.layer_norm.bias', 'rescnn_layers.2.cnn1.weight', 'rescnn_layers.2.cnn1.bias', 'rescnn_layers.2.cnn2.weight', 'rescnn_layers.2.cnn2.bias', 'rescnn_layers.2.layer_norm1.layer_norm.weight', 'rescnn_layers.2.layer_norm1.layer_norm.bias', 'rescnn_layers.2.layer_norm2.layer_norm.weight', 'rescnn_layers.2.layer_norm2.layer_norm.bias', 'fully_connected.weight', 'fully_connected.bias', 'birnn_layers.0.BiGRU.weight_ih_l0', 'birnn_layers.0.BiGRU.weight_hh_l0', 'birnn_layers.0.BiGRU.bias_ih_l0', 'birnn_layers.0.BiGRU.bias_hh_l0', 'birnn_layers.0.BiGRU.weight_ih_l0_reverse', 'birnn_layers.0.BiGRU.weight_hh_l0_reverse', 'birnn_layers.0.BiGRU.bias_ih_l0_reverse', 'birnn_layers.0.BiGRU.bias_hh_l0_reverse', 'birnn_layers.0.layer_norm.weight', 'birnn_layers.0.layer_norm.bias', 'birnn_layers.1.BiGRU.weight_ih_l0', 'birnn_layers.1.BiGRU.weight_hh_l0', 'birnn_layers.1.BiGRU.bias_ih_l0', 'birnn_layers.1.BiGRU.bias_hh_l0', 'birnn_layers.1.BiGRU.weight_ih_l0_reverse', 'birnn_layers.1.BiGRU.weight_hh_l0_reverse', 'birnn_layers.1.BiGRU.bias_ih_l0_reverse', 'birnn_layers.1.BiGRU.bias_hh_l0_reverse', 'birnn_layers.1.layer_norm.weight', 'birnn_layers.1.layer_norm.bias', 'birnn_layers.2.BiGRU.weight_ih_l0', 'birnn_layers.2.BiGRU.weight_hh_l0', 'birnn_layers.2.BiGRU.bias_ih_l0', 'birnn_layers.2.BiGRU.bias_hh_l0', 'birnn_layers.2.BiGRU.weight_ih_l0_reverse', 'birnn_layers.2.BiGRU.weight_hh_l0_reverse', 'birnn_layers.2.BiGRU.bias_ih_l0_reverse', 'birnn_layers.2.BiGRU.bias_hh_l0_reverse', 'birnn_layers.2.layer_norm.weight', 'birnn_layers.2.layer_norm.bias', 'birnn_layers.3.BiGRU.weight_ih_l0', 'birnn_layers.3.BiGRU.weight_hh_l0', 'birnn_layers.3.BiGRU.bias_ih_l0', 'birnn_layers.3.BiGRU.bias_hh_l0', 'birnn_layers.3.BiGRU.weight_ih_l0_reverse', 'birnn_layers.3.BiGRU.weight_hh_l0_reverse', 'birnn_layers.3.BiGRU.bias_ih_l0_reverse', 'birnn_layers.3.BiGRU.bias_hh_l0_reverse', 'birnn_layers.3.layer_norm.weight', 'birnn_layers.3.layer_norm.bias', 'birnn_layers.4.BiGRU.weight_ih_l0', 'birnn_layers.4.BiGRU.weight_hh_l0', 'birnn_layers.4.BiGRU.bias_ih_l0', 'birnn_layers.4.BiGRU.bias_hh_l0', 'birnn_layers.4.BiGRU.weight_ih_l0_reverse', 'birnn_layers.4.BiGRU.weight_hh_l0_reverse', 'birnn_layers.4.BiGRU.bias_ih_l0_reverse', 'birnn_layers.4.BiGRU.bias_hh_l0_reverse', 'birnn_layers.4.layer_norm.weight', 'birnn_layers.4.layer_norm.bias', 'classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias'])\n",
            "Train Epoch: 1 [0/100 (0%)]\tLoss: 2.302243\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.2089, Average CER: 0.680716 Average WER: 0.5635\n",
            "\n",
            "Train Epoch: 2 [0/100 (0%)]\tLoss: 2.282599\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.1171, Average CER: 0.638972 Average WER: 0.5557\n",
            "\n",
            "Train Epoch: 3 [0/100 (0%)]\tLoss: 2.157582\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.0972, Average CER: 0.681210 Average WER: 0.5696\n",
            "\n",
            "Train Epoch: 4 [0/100 (0%)]\tLoss: 2.227591\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.0303, Average CER: 0.536045 Average WER: 0.5379\n",
            "\n",
            "Train Epoch: 5 [0/100 (0%)]\tLoss: 2.119420\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.9862, Average CER: 0.571338 Average WER: 0.5361\n",
            "\n",
            "Train Epoch: 6 [0/100 (0%)]\tLoss: 2.004782\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.9517, Average CER: 0.544314 Average WER: 0.5274\n",
            "\n",
            "Train Epoch: 7 [0/100 (0%)]\tLoss: 1.982394\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.8210, Average CER: 0.501509 Average WER: 0.5039\n",
            "\n",
            "Train Epoch: 8 [0/100 (0%)]\tLoss: 1.960865\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.7346, Average CER: 0.487748 Average WER: 0.4926\n",
            "\n",
            "Train Epoch: 9 [0/100 (0%)]\tLoss: 1.870358\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.8690, Average CER: 0.444069 Average WER: 0.5421\n",
            "\n",
            "Train Epoch: 10 [0/100 (0%)]\tLoss: 1.942217\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.5577, Average CER: 0.423363 Average WER: 0.4667\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bTqmSurwWol"
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}