{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{"_uuid":"1fce1086-5ea5-46d6-b526-abc8bd7dd750","_cell_guid":"8fdae84a-3e95-4bf5-bd5b-866effc3595b","trusted":true}},{"cell_type":"markdown","source":"# Importing all neccessary libraries","metadata":{"_uuid":"124c36d9-b629-482b-a250-cfcc9154e19d","_cell_guid":"d5d3c866-164f-43a4-b6ac-c8bdb8fa2da4","trusted":true}},{"cell_type":"code","source":"import warnings\nwarnings. filterwarnings(\"ignore\")\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nfrom torchaudio import transforms as tfs\nimport numpy as np\nimport pandas as pd \nfrom torch.nn import Sequential as sq\nfrom torch.nn import Linear as lnr\nfrom torch import argmax as agm\nfrom torch.nn import Module as ml\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"5f6b3c78-2d63-458e-af9f-444c0b2945ae","_cell_guid":"11d4135a-a7a7-42e9-8166-9bb877d6c626","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-07T09:45:27.979343Z","iopub.execute_input":"2021-09-07T09:45:27.979703Z","iopub.status.idle":"2021-09-07T09:45:28.009294Z","shell.execute_reply.started":"2021-09-07T09:45:27.979673Z","shell.execute_reply":"2021-09-07T09:45:28.008232Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"/kaggle/input/slurred-speech/Test/1/12/1-12-0000.flac\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Evaluation Metrics implementation (WER and CER)","metadata":{"_uuid":"16406cb6-7151-48a0-bfd4-6e1e949e7ee8","_cell_guid":"eaacd2d5-6cce-45d9-8f24-2df17f68b288","trusted":true}},{"cell_type":"code","source":"import warnings\nwarnings. filterwarnings(\"ignore\")\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\n\ndef avg_wer(wer_scores, combined_ref_len):\n    return float(sum(wer_scores)) / float(combined_ref_len)\n\n\ndef _levenshtein_distance(ref, hyp):\n\n    m = len(ref)\n    n = len(hyp)\n\n    # special case\n    if ref == hyp:\n        return 0\n    if m == 0:\n        return n\n    if n == 0:\n        return m\n\n    if m < n:\n        ref, hyp = hyp, ref\n        m, n = n, m\n\n    # use O(min(m, n)) space\n    distance = np.zeros((2, n + 1), dtype=np.int32)\n\n    # initialize distance matrix\n    for j in range(0,n + 1):\n        distance[0][j] = j\n\n    # calculate levenshtein distance\n    for i in range(1, m + 1):\n        prev_row_idx = (i - 1) % 2\n        cur_row_idx = i % 2\n        distance[cur_row_idx][0] = i\n        for j in range(1, n + 1):\n            if ref[i - 1] == hyp[j - 1]:\n                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n            else:\n                s_num = distance[prev_row_idx][j - 1] + 1\n                i_num = distance[cur_row_idx][j - 1] + 1\n                d_num = distance[prev_row_idx][j] + 1\n                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n\n    return distance[m % 2][n]\n\n\ndef word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    ref_words = reference.split(delimiter)\n    hyp_words = hypothesis.split(delimiter)\n\n    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n    return float(edit_distance), len(ref_words)\n\n\ndef char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n    if ignore_case == True:\n        reference = reference.lower()\n        hypothesis = hypothesis.lower()\n\n    join_char = ' '\n    if remove_space == True:\n        join_char = ''\n\n    reference = join_char.join(filter(None, reference.split(' ')))\n    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n\n    edit_distance = _levenshtein_distance(reference, hypothesis)\n    return float(edit_distance), len(reference)\n\n\ndef wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n  \n    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n                                         delimiter)\n\n    if ref_len == 0:\n        raise ValueError(\"Reference's word number should be greater than 0.\")\n\n    wer = float(edit_distance) / ref_len\n    return wer\n\n\ndef cer(reference, hypothesis, ignore_case=False, remove_space=False):\n \n    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n                                         remove_space)\n\n    if ref_len == 0:\n        raise ValueError(\"Length of reference should be greater than 0.\")\n\n    cer = float(edit_distance) / ref_len\n    return cer\n\nclass TextTransform:\n    def __init__(self):\n        self.char_map={\"'\": 0, '': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n        self.index_map={0: \"'\", 1: ' ', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n        \n    def text_to_int(self, text):\n        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n        int_sequence = []\n        for c in text:\n            if c == ' ':\n                ch = self.char_map['']\n            else:\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    \n    def int_to_text(self, labels):\n        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string).replace('', ' ')\n\ntrain_audio_transforms = nn.Sequential(\n    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n    torchaudio.transforms.TimeMasking(time_mask_param=100)\n)\n\nvalid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n\ntext_transform = TextTransform()\n\ndef data_processing(data, data_type=\"train\"):\n    spectrograms = []\n    labels = []\n    input_lengths = []\n    label_lengths = []\n    for (waveform, _, utterance, _, _, _) in data:\n        if data_type == 'train':\n            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        elif data_type == 'valid':\n            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n        else:\n            raise Exception('data_type should be train or valid')\n        spectrograms.append(spec)\n        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n        labels.append(label)\n        input_lengths.append(spec.shape[0]//2)\n        label_lengths.append(len(label))\n\n    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n\n    return spectrograms, labels, input_lengths, label_lengths\n\n\ndef GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n\targ_maxes = torch.argmax(output, dim=2)\n\tdecodes = []\n\ttargets = []\n\tfor i, args in enumerate(arg_maxes):\n\t\tdecode = []\n\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n\t\tfor j, index in enumerate(args):\n\t\t\tif index != blank_label:\n\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n\t\t\t\t\tcontinue\n\t\t\t\tdecode.append(index.item())\n\t\tdecodes.append(text_transform.int_to_text(decode))\n\treturn decodes, targets","metadata":{"_uuid":"35905574-3ccd-4656-a63e-e003ced0fa7a","_cell_guid":"02dc287c-5658-4995-ba90-8833230f15b1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-07T09:45:32.649128Z","iopub.execute_input":"2021-09-07T09:45:32.649509Z","iopub.status.idle":"2021-09-07T09:45:32.717319Z","shell.execute_reply.started":"2021-09-07T09:45:32.649476Z","shell.execute_reply":"2021-09-07T09:45:32.716454Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"28544719-2e9f-4032-9cc5-3a6529aa26d5","_cell_guid":"35180eab-1527-4fd5-bae6-aec50f2eefae","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-07T09:45:39.393941Z","iopub.execute_input":"2021-09-07T09:45:39.394321Z","iopub.status.idle":"2021-09-07T09:45:40.188548Z","shell.execute_reply.started":"2021-09-07T09:45:39.394284Z","shell.execute_reply":"2021-09-07T09:45:40.187447Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Tue Sep  7 09:45:40 2021       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    33W / 250W |  16085MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# CNN layer","metadata":{"_uuid":"49e027bb-c519-4316-887f-66572def367b","_cell_guid":"7794c0e4-b823-4837-bb2c-b0fa56e8b0e0","trusted":true}},{"cell_type":"code","source":"class CNNLayerNorm(nn.Module):\n    \"\"\"Layer normalization built for cnns input\"\"\"\n    def __init__(self, n_feats):\n        super(CNNLayerNorm, self).__init__()\n        self.layer_norm = nn.LayerNorm(n_feats)\n\n    def forward(self, x):\n        # x (batch, channel, feature, time)\n        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n        x = self.layer_norm(x)\n        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n\n\nclass ResidualCNN(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n        super(ResidualCNN, self).__init__()\n\n        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n        self.layer_norm1 = CNNLayerNorm(n_feats)\n        self.layer_norm2 = CNNLayerNorm(n_feats)\n\n    def forward(self, x):\n        residual = x  # (batch, channel, feature, time)\n        x = self.layer_norm1(x)\n        x = F.gelu(x)\n        x = self.dropout1(x)\n        x = self.cnn1(x)\n        x = self.layer_norm2(x)\n        x = F.gelu(x)\n        x = self.dropout2(x)\n        x = self.cnn2(x)\n        x += residual\n        return x # (batch, channel, feature, time)\n\n\n","metadata":{"_uuid":"7ccd8f61-fd63-4f88-9ee7-fb18149214bf","_cell_guid":"c2da284b-5059-47a5-a9e2-6d013b8ddee1","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-07T09:45:42.993517Z","iopub.execute_input":"2021-09-07T09:45:42.993906Z","iopub.status.idle":"2021-09-07T09:45:43.004735Z","shell.execute_reply.started":"2021-09-07T09:45:42.993854Z","shell.execute_reply":"2021-09-07T09:45:43.003445Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Gated Recurrent Unit layer","metadata":{"_uuid":"c05e08ec-c55d-41db-a24d-65b328ca868b","_cell_guid":"9995abf0-33ed-41b6-90a3-6bb44effdb90","trusted":true}},{"cell_type":"markdown","source":"","metadata":{"_uuid":"166e2955-f916-46d1-b8e6-b73894a7e8a4","_cell_guid":"74b10ab6-95e7-4359-93e9-14961927c6c8","trusted":true}},{"cell_type":"code","source":"class BidirectionalGRU(nn.Module):\n\n    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n        super(BidirectionalGRU, self).__init__()\n\n        self.BiGRU = nn.GRU(\n            input_size=rnn_dim, hidden_size=hidden_size,\n            num_layers=1, batch_first=batch_first, bidirectional=True)\n        self.layer_norm = nn.LayerNorm(rnn_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = F.gelu(x)\n        x, _ = self.BiGRU(x)\n        x = self.dropout(x)\n        return x\n","metadata":{"_uuid":"1c3ad2c3-b861-4677-bad2-bb8d0928c87d","_cell_guid":"b7cf2e7f-8c9f-4e9b-b2b8-2d0fad66a696","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-07T09:45:47.003447Z","iopub.execute_input":"2021-09-07T09:45:47.003784Z","iopub.status.idle":"2021-09-07T09:45:47.011465Z","shell.execute_reply.started":"2021-09-07T09:45:47.003751Z","shell.execute_reply":"2021-09-07T09:45:47.010428Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Model architecture","metadata":{"_uuid":"c071854d-2e5c-4b40-ac88-ec1e6bb42d92","_cell_guid":"49420e26-f28a-4b3c-97be-4171df99e922","trusted":true}},{"cell_type":"code","source":"\n\nclass SpeechRecognitionModel(nn.Module):\n    \n    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n        super(SpeechRecognitionModel, self).__init__()\n        n_feats = n_feats//2\n        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n\n        # n residual cnn layers with filter size of 32\n        self.rescnn_layers = nn.Sequential(*[\n            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n            for _ in range(n_cnn_layers)\n        ])\n        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n        self.birnn_layers = nn.Sequential(*[\n            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n            for i in range(n_rnn_layers)\n        ])\n        self.classifier = nn.Sequential(\n            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(rnn_dim, n_class)\n        )\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rescnn_layers(x)\n        sizes = x.size()\n        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n        x = x.transpose(1, 2) # (batch, time, feature)\n        x = self.fully_connected(x)\n        x = self.birnn_layers(x)\n        x = self.classifier(x)\n        return x","metadata":{"_uuid":"83daee48-464a-46d2-b77b-cb092ecd3509","_cell_guid":"3a59c66c-69a8-46cb-9531-34559fb572b4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-07T09:45:50.343030Z","iopub.execute_input":"2021-09-07T09:45:50.343388Z","iopub.status.idle":"2021-09-07T09:45:50.353057Z","shell.execute_reply.started":"2021-09-07T09:45:50.343349Z","shell.execute_reply":"2021-09-07T09:45:50.352259Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Training and Testing Code","metadata":{}},{"cell_type":"code","source":"class IterMeter(object):\n    \"\"\"keeps track of total iterations\"\"\"\n    def __init__(self):\n        self.val = 0\n\n    def step(self):\n        self.val += 1\n\n    def get(self):\n        return self.val\n\n\ndef train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n    model.train()\n    data_len = len(train_loader.dataset)\n\n    for batch_idx, _data in enumerate(train_loader):\n        spectrograms, labels, input_lengths, label_lengths = _data \n        spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        output = model(spectrograms)  # (batch, time, n_class)\n        output = F.log_softmax(output, dim=2)\n        output = output.transpose(0, 1) # (time, batch, n_class)\n\n        loss = criterion(output, labels, input_lengths, label_lengths)\n        loss.backward()\n\n    \n        optimizer.step()\n        scheduler.step()\n        iter_meter.step()\n        if batch_idx % 100 == 0 or batch_idx == data_len:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(spectrograms), data_len,\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n\n            output = model(spectrograms)  # (batch, time, n_class)\n            output = F.log_softmax(output, dim=2)\n            output = output.transpose(0, 1) # (time, batch, n_class)\n\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n\n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)\n  \n    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:45:54.190594Z","iopub.execute_input":"2021-09-07T09:45:54.190962Z","iopub.status.idle":"2021-09-07T09:45:54.205474Z","shell.execute_reply.started":"2021-09-07T09:45:54.190925Z","shell.execute_reply":"2021-09-07T09:45:54.204483Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"learning_rate = 5e-4\nbatch_size = 5\nepochs = 2\ntrain_url = \"train-clean-100\"\ntest_url = \"test-clean\"\n\nhparams = {\n    \"n_cnn_layers\": 3,\n    \"n_rnn_layers\": 5,\n    \"rnn_dim\": 512,\n    \"n_class\": 29,\n    \"n_feats\": 128,\n    \"stride\":2,\n    \"dropout\": 0.1,\n    \"learning_rate\": learning_rate,\n    \"batch_size\": batch_size,\n    \"epochs\": epochs\n}\n\nuse_cuda = torch.cuda.is_available()\ntorch.manual_seed(7)\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nif not os.path.isdir(\"./data\"):\n    os.makedirs(\"./data\")\n\ntrain_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\ntest_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n\nkwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\ntrain_loader = data.DataLoader(dataset=train_dataset,\n                            batch_size=hparams['batch_size'],\n                            shuffle=True,\n                            collate_fn=lambda x: data_processing(x, 'train'),\n                            **kwargs)\ntest_loader = data.DataLoader(dataset=test_dataset,\n                            batch_size=hparams['batch_size'],\n                            shuffle=False,\n                            collate_fn=lambda x: data_processing(x, 'valid'),\n                            **kwargs)\n\nmodel = SpeechRecognitionModel(\n    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n    ).to(device)\n\nprint(model)\nprint('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n\noptimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\ncriterion = nn.CTCLoss(blank=28).to(device)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n                                        steps_per_epoch=int(len(train_loader)),\n                                        epochs=hparams['epochs'],\n                                        anneal_strategy='linear')\n\niter_meter = IterMeter()\nfor epoch in range(1, epochs + 1):\n    train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n    test(model, device, test_loader, criterion, epoch, iter_meter)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T09:45:59.495311Z","iopub.execute_input":"2021-09-07T09:45:59.495654Z","iopub.status.idle":"2021-09-07T11:35:02.737148Z","shell.execute_reply.started":"2021-09-07T09:45:59.495621Z","shell.execute_reply":"2021-09-07T11:35:02.736114Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"SpeechRecognitionModel(\n  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (rescnn_layers): Sequential(\n    (0): ResidualCNN(\n      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n      (layer_norm1): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (1): ResidualCNN(\n      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n      (layer_norm1): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (2): ResidualCNN(\n      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n      (layer_norm1): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n  (birnn_layers): Sequential(\n    (0): BidirectionalGRU(\n      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (1): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (2): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (3): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (4): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=1024, out_features=512, bias=True)\n    (1): GELU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=512, out_features=29, bias=True)\n  )\n)\nNum Model Parameters 23705373\nTrain Epoch: 1 [0/28539 (0%)]\tLoss: 6.903579\nTrain Epoch: 1 [500/28539 (2%)]\tLoss: 2.993040\nTrain Epoch: 1 [1000/28539 (4%)]\tLoss: 2.837706\nTrain Epoch: 1 [1500/28539 (5%)]\tLoss: 2.887584\nTrain Epoch: 1 [2000/28539 (7%)]\tLoss: 2.876701\nTrain Epoch: 1 [2500/28539 (9%)]\tLoss: 2.849155\nTrain Epoch: 1 [3000/28539 (11%)]\tLoss: 2.839344\nTrain Epoch: 1 [3500/28539 (12%)]\tLoss: 2.845058\nTrain Epoch: 1 [4000/28539 (14%)]\tLoss: 2.888634\nTrain Epoch: 1 [4500/28539 (16%)]\tLoss: 2.863554\nTrain Epoch: 1 [5000/28539 (18%)]\tLoss: 2.885831\nTrain Epoch: 1 [5500/28539 (19%)]\tLoss: 2.905257\nTrain Epoch: 1 [6000/28539 (21%)]\tLoss: 2.853825\nTrain Epoch: 1 [6500/28539 (23%)]\tLoss: 2.830079\nTrain Epoch: 1 [7000/28539 (25%)]\tLoss: 2.804203\nTrain Epoch: 1 [7500/28539 (26%)]\tLoss: 2.814881\nTrain Epoch: 1 [8000/28539 (28%)]\tLoss: 2.774024\nTrain Epoch: 1 [8500/28539 (30%)]\tLoss: 2.646364\nTrain Epoch: 1 [9000/28539 (32%)]\tLoss: 2.454883\nTrain Epoch: 1 [9500/28539 (33%)]\tLoss: 2.327569\nTrain Epoch: 1 [10000/28539 (35%)]\tLoss: 2.215783\nTrain Epoch: 1 [10500/28539 (37%)]\tLoss: 2.209273\nTrain Epoch: 1 [11000/28539 (39%)]\tLoss: 2.073301\nTrain Epoch: 1 [11500/28539 (40%)]\tLoss: 1.976873\nTrain Epoch: 1 [12000/28539 (42%)]\tLoss: 1.936437\nTrain Epoch: 1 [12500/28539 (44%)]\tLoss: 2.044173\nTrain Epoch: 1 [13000/28539 (46%)]\tLoss: 1.944897\nTrain Epoch: 1 [13500/28539 (47%)]\tLoss: 1.730704\nTrain Epoch: 1 [14000/28539 (49%)]\tLoss: 1.576369\nTrain Epoch: 1 [14500/28539 (51%)]\tLoss: 1.777175\nTrain Epoch: 1 [15000/28539 (53%)]\tLoss: 1.684871\nTrain Epoch: 1 [15500/28539 (54%)]\tLoss: 1.667748\nTrain Epoch: 1 [16000/28539 (56%)]\tLoss: 1.644173\nTrain Epoch: 1 [16500/28539 (58%)]\tLoss: 1.693885\nTrain Epoch: 1 [17000/28539 (60%)]\tLoss: 1.826122\nTrain Epoch: 1 [17500/28539 (61%)]\tLoss: 1.574871\nTrain Epoch: 1 [18000/28539 (63%)]\tLoss: 1.727947\nTrain Epoch: 1 [18500/28539 (65%)]\tLoss: 1.606231\nTrain Epoch: 1 [19000/28539 (67%)]\tLoss: 1.514162\nTrain Epoch: 1 [19500/28539 (68%)]\tLoss: 1.568282\nTrain Epoch: 1 [20000/28539 (70%)]\tLoss: 1.422759\nTrain Epoch: 1 [20500/28539 (72%)]\tLoss: 1.393984\nTrain Epoch: 1 [21000/28539 (74%)]\tLoss: 1.618255\nTrain Epoch: 1 [21500/28539 (75%)]\tLoss: 1.666287\nTrain Epoch: 1 [22000/28539 (77%)]\tLoss: 1.450228\nTrain Epoch: 1 [22500/28539 (79%)]\tLoss: 1.395637\nTrain Epoch: 1 [23000/28539 (81%)]\tLoss: 1.466214\nTrain Epoch: 1 [23500/28539 (82%)]\tLoss: 1.316899\nTrain Epoch: 1 [24000/28539 (84%)]\tLoss: 1.384712\nTrain Epoch: 1 [24500/28539 (86%)]\tLoss: 1.698025\nTrain Epoch: 1 [25000/28539 (88%)]\tLoss: 1.440167\nTrain Epoch: 1 [25500/28539 (89%)]\tLoss: 1.368115\nTrain Epoch: 1 [26000/28539 (91%)]\tLoss: 1.386709\nTrain Epoch: 1 [26500/28539 (93%)]\tLoss: 1.386220\nTrain Epoch: 1 [27000/28539 (95%)]\tLoss: 1.247020\nTrain Epoch: 1 [27500/28539 (96%)]\tLoss: 1.079716\nTrain Epoch: 1 [28000/28539 (98%)]\tLoss: 1.626849\nTrain Epoch: 1 [28500/28539 (100%)]\tLoss: 1.217542\n\nevaluating...\nTest set: Average loss: 1.1182, Average CER: 0.266134 Average WER: 0.3102\n\nTrain Epoch: 2 [0/28539 (0%)]\tLoss: 1.330486\nTrain Epoch: 2 [500/28539 (2%)]\tLoss: 1.435540\nTrain Epoch: 2 [1000/28539 (4%)]\tLoss: 1.256594\nTrain Epoch: 2 [1500/28539 (5%)]\tLoss: 1.567772\nTrain Epoch: 2 [2000/28539 (7%)]\tLoss: 1.185688\nTrain Epoch: 2 [2500/28539 (9%)]\tLoss: 1.066938\nTrain Epoch: 2 [3000/28539 (11%)]\tLoss: 1.309850\nTrain Epoch: 2 [3500/28539 (12%)]\tLoss: 1.334300\nTrain Epoch: 2 [4000/28539 (14%)]\tLoss: 1.202369\nTrain Epoch: 2 [4500/28539 (16%)]\tLoss: 1.288700\nTrain Epoch: 2 [5000/28539 (18%)]\tLoss: 1.304150\nTrain Epoch: 2 [5500/28539 (19%)]\tLoss: 1.106888\nTrain Epoch: 2 [6000/28539 (21%)]\tLoss: 1.130301\nTrain Epoch: 2 [6500/28539 (23%)]\tLoss: 1.408867\nTrain Epoch: 2 [7000/28539 (25%)]\tLoss: 1.159099\nTrain Epoch: 2 [7500/28539 (26%)]\tLoss: 1.276379\nTrain Epoch: 2 [8000/28539 (28%)]\tLoss: 1.266896\nTrain Epoch: 2 [8500/28539 (30%)]\tLoss: 1.195186\nTrain Epoch: 2 [9000/28539 (32%)]\tLoss: 1.187417\nTrain Epoch: 2 [9500/28539 (33%)]\tLoss: 1.143909\nTrain Epoch: 2 [10000/28539 (35%)]\tLoss: 1.076864\nTrain Epoch: 2 [10500/28539 (37%)]\tLoss: 1.216958\nTrain Epoch: 2 [11000/28539 (39%)]\tLoss: 0.994317\nTrain Epoch: 2 [11500/28539 (40%)]\tLoss: 1.257738\nTrain Epoch: 2 [12000/28539 (42%)]\tLoss: 1.103883\nTrain Epoch: 2 [12500/28539 (44%)]\tLoss: 1.088572\nTrain Epoch: 2 [13000/28539 (46%)]\tLoss: 1.075297\nTrain Epoch: 2 [13500/28539 (47%)]\tLoss: 1.182057\nTrain Epoch: 2 [14000/28539 (49%)]\tLoss: 1.003504\nTrain Epoch: 2 [14500/28539 (51%)]\tLoss: 1.286355\nTrain Epoch: 2 [15000/28539 (53%)]\tLoss: 1.203347\nTrain Epoch: 2 [15500/28539 (54%)]\tLoss: 1.147613\nTrain Epoch: 2 [16000/28539 (56%)]\tLoss: 1.245651\nTrain Epoch: 2 [16500/28539 (58%)]\tLoss: 1.027859\nTrain Epoch: 2 [17000/28539 (60%)]\tLoss: 1.497562\nTrain Epoch: 2 [17500/28539 (61%)]\tLoss: 0.939938\nTrain Epoch: 2 [18000/28539 (63%)]\tLoss: 1.037313\nTrain Epoch: 2 [18500/28539 (65%)]\tLoss: 0.970310\nTrain Epoch: 2 [19000/28539 (67%)]\tLoss: 1.196396\nTrain Epoch: 2 [19500/28539 (68%)]\tLoss: 1.114947\nTrain Epoch: 2 [20000/28539 (70%)]\tLoss: 1.350462\nTrain Epoch: 2 [20500/28539 (72%)]\tLoss: 1.193184\nTrain Epoch: 2 [21000/28539 (74%)]\tLoss: 1.103393\nTrain Epoch: 2 [21500/28539 (75%)]\tLoss: 1.018087\nTrain Epoch: 2 [22000/28539 (77%)]\tLoss: 1.005365\nTrain Epoch: 2 [22500/28539 (79%)]\tLoss: 1.015692\nTrain Epoch: 2 [23000/28539 (81%)]\tLoss: 1.003248\nTrain Epoch: 2 [24000/28539 (84%)]\tLoss: 0.997269\nTrain Epoch: 2 [24500/28539 (86%)]\tLoss: 1.040507\nTrain Epoch: 2 [25000/28539 (88%)]\tLoss: 0.969322\nTrain Epoch: 2 [25500/28539 (89%)]\tLoss: 1.263573\nTrain Epoch: 2 [26000/28539 (91%)]\tLoss: 1.015066\nTrain Epoch: 2 [26500/28539 (93%)]\tLoss: 1.336682\nTrain Epoch: 2 [27000/28539 (95%)]\tLoss: 1.032019\nTrain Epoch: 2 [27500/28539 (96%)]\tLoss: 0.924982\nTrain Epoch: 2 [28000/28539 (98%)]\tLoss: 1.150139\nTrain Epoch: 2 [28500/28539 (100%)]\tLoss: 0.924037\n\nevaluating...\nTest set: Average loss: 0.8547, Average CER: 0.204964 Average WER: 0.2421\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Creation of Custom Dataset for slurred speech","metadata":{}},{"cell_type":"code","source":"import os\nfrom typing import Tuple, Union\nfrom pathlib import Path\nimport torchaudio\nfrom torch import Tensor\nfrom torch.utils.data import Dataset\n\n\ndef load_librispeech_item(fileid,path,ext_audio,ext_txt):\n    speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n\n    file_text = speaker_id + \"-\" + chapter_id + ext_txt\n    file_text = os.path.join(path, speaker_id, chapter_id, file_text)\n\n    fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n    file_audio = fileid_audio + ext_audio\n    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n\n    waveform, sample_rate = torchaudio.load(file_audio)\n\n    with open(file_text) as ft:\n        for line in ft:\n            fileid_text, utterance = line.strip().split(\" \", 1)\n            if fileid_audio == fileid_text:\n                break\n        else:\n            raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n\n    return (waveform,sample_rate,utterance,int(speaker_id),int(chapter_id),int(utterance_id),)\n\n\nclass TestDataset(Dataset): \n    _ext_txt = \".trans.txt\"\n    _ext_audio = \".flac\"\n    def __init__(self):\n        self._path=\"../input/slurredspeech/Test\"\n        self._walker=['1-12-0000']\n        \n    def __getitem__(self, n):\n        fileid = self._walker[n]\n        return load_librispeech_item(fileid, self._path, self._ext_audio, self._ext_txt)\n\n    def __len__(self):\n        return len(self._walker)   \n    \n    \nt=TestDataset()\nprint(t.__dict__)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T11:41:37.992773Z","iopub.execute_input":"2021-09-07T11:41:37.993146Z","iopub.status.idle":"2021-09-07T11:41:38.009768Z","shell.execute_reply.started":"2021-09-07T11:41:37.993115Z","shell.execute_reply":"2021-09-07T11:41:38.008770Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"2\nSpeechRecognitionModel(\n  (cnn): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n  (rescnn_layers): Sequential(\n    (0): ResidualCNN(\n      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n      (layer_norm1): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (1): ResidualCNN(\n      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n      (layer_norm1): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (2): ResidualCNN(\n      (cnn1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (cnn2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (dropout1): Dropout(p=0.1, inplace=False)\n      (dropout2): Dropout(p=0.1, inplace=False)\n      (layer_norm1): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n      (layer_norm2): CNNLayerNorm(\n        (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (fully_connected): Linear(in_features=2048, out_features=512, bias=True)\n  (birnn_layers): Sequential(\n    (0): BidirectionalGRU(\n      (BiGRU): GRU(512, 512, batch_first=True, bidirectional=True)\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (1): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (2): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (3): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (4): BidirectionalGRU(\n      (BiGRU): GRU(1024, 512, bidirectional=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=1024, out_features=512, bias=True)\n    (1): GELU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=512, out_features=29, bias=True)\n  )\n)\n{'_path': '../input/slurredspeech/Test', '_walker': ['1-12-0000']}\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = data.DataLoader(dataset=t,batch_size=hparams['batch_size'],shuffle=False,collate_fn=lambda x: data_processing(x, 'valid'),**kwargs)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T11:41:44.714800Z","iopub.execute_input":"2021-09-07T11:41:44.715117Z","iopub.status.idle":"2021-09-07T11:41:44.720937Z","shell.execute_reply.started":"2021-09-07T11:41:44.715087Z","shell.execute_reply":"2021-09-07T11:41:44.719922Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def t_test(model, device, test_loader, criterion, epoch, iter_meter):\n    print('\\nevaluating...')\n    model.eval()\n    test_loss = 0\n    test_cer, test_wer = [], []\n\n    with torch.no_grad():\n        for i, _data in enumerate(test_loader):\n            spectrograms, labels, input_lengths, label_lengths = _data \n            spectrograms, labels = spectrograms.to(device), labels.to(device)\n            print(labels)\n            output = model(spectrograms)  # (batch, time, n_class)\n            print(output)\n            output = F.log_softmax(output, dim=2)\n            print(output)\n            output = output.transpose(0, 1) # (time, batch, n_class)\n            print(output)\n            loss = criterion(output, labels, input_lengths, label_lengths)\n            test_loss += loss.item() / len(test_loader)\n\n            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n            print(\"#\",decoded_preds)\n            print('*',decoded_targets)\n            for j in range(len(decoded_preds)):\n                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n\n\n    avg_cer = sum(test_cer)/len(test_cer)\n    avg_wer = sum(test_wer)/len(test_wer)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T11:48:12.247556Z","iopub.execute_input":"2021-09-07T11:48:12.247901Z","iopub.status.idle":"2021-09-07T11:48:12.259266Z","shell.execute_reply.started":"2021-09-07T11:48:12.247867Z","shell.execute_reply":"2021-09-07T11:48:12.258181Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"t_test(model, device, test_loader, criterion, epoch, iter_meter)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T11:48:19.175156Z","iopub.execute_input":"2021-09-07T11:48:19.175582Z","iopub.status.idle":"2021-09-07T11:48:20.265105Z","shell.execute_reply.started":"2021-09-07T11:48:19.175549Z","shell.execute_reply":"2021-09-07T11:48:20.264138Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\nevaluating...\ntensor([[21.,  9., 10., 20.,  1., 13., 10., 21., 21., 13.,  6.,  1., 24., 16.,\n         19., 12.,  1., 24.,  2., 20.,  1.,  7., 10., 15., 10., 20.,  9.,  6.,\n          5.,  1., 10., 15.,  1., 21.,  9.,  6.,  1., 26.,  6.,  2., 19.,  1.,\n          6., 10.,  8.,  9., 21.,  6.,  6., 15.,  1., 16.,  1., 21.,  9., 19.,\n          6.,  6.,  1.,  2., 15.,  5.,  1., 10., 15., 21.,  6., 15.,  5.,  6.,\n          5.,  1.,  7., 16., 19.,  1., 10., 14., 14.,  6.,  5., 10.,  2., 21.,\n          6.,  1., 17., 22.,  3., 13., 10.,  4.,  2., 21., 10., 16., 15.,  1.,\n         10., 21.,  1., 24.,  2., 20.,  1.,  5., 10., 20., 17., 16., 20.,  6.,\n          5.,  1., 16.,  7.,  1., 21., 16.,  1.,  2.,  1.,  3., 16., 16., 12.,\n         20.,  6., 13., 13.,  6., 19.,  1., 10., 21.,  1., 24.,  2., 20.,  1.,\n          6., 23.,  6., 15.,  1.,  2.,  5., 23.,  6., 19., 21., 10., 20.,  6.,\n          5.]], device='cuda:0')\ntensor([[[-2.0813, -4.5710, -2.4890,  ..., -4.1464, -7.1879, 11.4014],\n         [-2.2574, -3.2308, -2.5983,  ..., -4.4052, -7.8284, 11.8758],\n         [-2.4563, -2.8624, -2.5206,  ..., -4.6603, -8.1636, 12.1857],\n         ...,\n         [-2.4334, -0.7977, -2.8548,  ..., -4.6242, -8.2255, 12.1757],\n         [-2.6359,  1.8637, -2.2233,  ..., -4.7712, -8.0806, 11.4396],\n         [-2.9587,  5.4103, -1.5872,  ..., -4.7564, -8.7428, 10.1277]]],\n       device='cuda:0')\ntensor([[[-1.3483e+01, -1.5973e+01, -1.3891e+01,  ..., -1.5548e+01,\n          -1.8590e+01, -2.4185e-04],\n         [-1.4133e+01, -1.5107e+01, -1.4474e+01,  ..., -1.6281e+01,\n          -1.9704e+01, -9.8581e-05],\n         [-1.4642e+01, -1.5048e+01, -1.4706e+01,  ..., -1.6846e+01,\n          -2.0349e+01, -5.6504e-05],\n         ...,\n         [-1.4609e+01, -1.2973e+01, -1.5030e+01,  ..., -1.6800e+01,\n          -2.0401e+01, -5.3762e-05],\n         [-1.4076e+01, -9.5760e+00, -1.3663e+01,  ..., -1.6211e+01,\n          -1.9520e+01, -1.4507e-04],\n         [-1.3096e+01, -4.7265e+00, -1.1724e+01,  ..., -1.4893e+01,\n          -1.8880e+01, -9.0846e-03]]], device='cuda:0')\ntensor([[[-1.3483e+01, -1.5973e+01, -1.3891e+01,  ..., -1.5548e+01,\n          -1.8590e+01, -2.4185e-04]],\n\n        [[-1.4133e+01, -1.5107e+01, -1.4474e+01,  ..., -1.6281e+01,\n          -1.9704e+01, -9.8581e-05]],\n\n        [[-1.4642e+01, -1.5048e+01, -1.4706e+01,  ..., -1.6846e+01,\n          -2.0349e+01, -5.6504e-05]],\n\n        ...,\n\n        [[-1.4609e+01, -1.2973e+01, -1.5030e+01,  ..., -1.6800e+01,\n          -2.0401e+01, -5.3762e-05]],\n\n        [[-1.4076e+01, -9.5760e+00, -1.3663e+01,  ..., -1.6211e+01,\n          -1.9520e+01, -1.4507e-04]],\n\n        [[-1.3096e+01, -4.7265e+00, -1.1724e+01,  ..., -1.4893e+01,\n          -1.8880e+01, -9.0846e-03]]], device='cuda:0')\n# [' t h i s   l i t t l e   w o r k   w a s   f i n i s h e d   i n t h e   y a r e   a t e e n   o t e r y   a n d   i n t e n d e n   f o r   a   m e t y   a p u b r c a t i o n   i t   w a s   d i s p o s e d   a f   t o   a   r o k s e l l e r   i t   w a s   e v e n t   a v r t i s e ']\n* [' t h i s   l i t t l e   w o r k   w a s   f i n i s h e d   i n   t h e   y e a r   e i g h t e e n   o   t h r e e   a n d   i n t e n d e d   f o r   i m m e d i a t e   p u b l i c a t i o n   i t   w a s   d i s p o s e d   o f   t o   a   b o o k s e l l e r   i t   w a s   e v e n   a d v e r t i s e d ']\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}